{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the delays in a morning peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes a all the raw data from a single delay, and determines the number of delays, and the worst delay for a set time period in the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted file 20190123.zip\n"
     ]
    }
   ],
   "source": [
    "file_name = '20190123.zip'\n",
    "with open(file_name, \"rb\") as f:\n",
    "    z = zipfile.ZipFile(io.BytesIO(f.read()))\n",
    "\n",
    "z.extractall()\n",
    "print(\"Extracted file \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few directories to dive into before we get to the good stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 720 delay files\n",
      "The timetable files are: home/pi/sydney-transport-tracker/data/raw/20190123/stop_times.txt, home/pi/sydney-transport-tracker/data/raw/20190123/shapes.txt, home/pi/sydney-transport-tracker/data/raw/20190123/stops.txt, home/pi/sydney-transport-tracker/data/raw/20190123/calendar.txt, home/pi/sydney-transport-tracker/data/raw/20190123/trips.txt, home/pi/sydney-transport-tracker/data/raw/20190123/agency.txt, home/pi/sydney-transport-tracker/data/raw/20190123/routes.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "data_path = 'home/pi/sydney-transport-tracker/data/raw/20190123/'\n",
    "delay_files_count = len(glob.glob(data_path + '*.pickle'))\n",
    "print('We have ' + str(delay_files_count) + ' delay files')\n",
    "timetable_files = glob.glob(data_path + '*.txt')\n",
    "print('The timetable files are: ' + ', '.join(timetable_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What services are running today?\n",
    "The services are determined in `calendar.txt`, with those service IDs used to filter out everything in `trips.txt`. From there we have trip IDs, which can be used to filter out the scheduled stop times in `stop_times.txt`.\n",
    "\n",
    "`calendar.txt` -> `service_id` -> `trips.txt` -> `trip_id` -> `stop_times.txt` -> `arrival_time`, `departure_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todays services = 1260.122.112, 1260.122.116, 1260.122.120, 1260.122.124, 1260.122.16, 1260.122.20, 1260.122.24, 1260.122.28, 1260.122.48, 1260.122.52, 1260.122.56, 1260.122.60, 1260.122.80, 1260.122.84, 1260.122.88, 1260.122.92\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "day_of_analysis = 'wednesday'\n",
    "date_of_analysis = datetime.datetime.strptime('20190123', \"%Y%m%d\").date()\n",
    "todays_services = []\n",
    "\n",
    "with open(data_path + 'calendar.txt', mode='r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if row[day_of_analysis] == '1':\n",
    "            start_date = datetime.datetime.strptime(row['start_date'], \"%Y%m%d\").date()\n",
    "            end_date = datetime.datetime.strptime(row['end_date'], \"%Y%m%d\").date()\n",
    "            if start_date <= date_of_analysis <= end_date:\n",
    "                todays_services.append(row['service_id'])\n",
    "\n",
    "print(\"Todays services = \" + ', '.join(todays_services))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       route_id    service_id                         trip_id  trip_short_name\n",
      "0        BNK_2a    1260.122.4    1--A.1260.122.4.M.8.55188158              NaN\n",
      "1        BNK_2a   1260.122.48   1--A.1260.122.48.M.8.55188157              NaN\n",
      "2        BNK_2a   1260.122.64   1--A.1260.122.64.M.8.55188157              NaN\n",
      "3        BNK_2a    1260.122.8    1--A.1260.122.8.M.8.55188158              NaN\n",
      "4      RTTA_DEF  1603.103.128  1--A.1603.103.128.M.8.54724494              NaN\n",
      "...         ...           ...                             ...              ...\n",
      "57308   CTY_W2a    1620.100.2    WT28.1620.100.2.X.5.55042064              NaN\n",
      "57309   CTY_W2a    1620.100.4    WT28.1620.100.4.X.5.55042062              NaN\n",
      "57310   CTY_W2a   483.101.120   WT28.483.101.120.X.5.55277012              NaN\n",
      "57311   CTY_W2a    487.111.60    WT28.487.111.60.X.5.55310994              NaN\n",
      "57312   CTY_W2a    487.111.64    WT28.487.111.64.X.5.55310994              NaN\n",
      "\n",
      "[57313 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_trips = pd.read_csv(data_path + 'trips.txt',\n",
    "                       header=0,\n",
    "                       encoding='utf-8-sig',\n",
    "                       usecols=[\"route_id\", \"service_id\", \"trip_id\", \"trip_short_name\"])\n",
    "df_filtered_trips = df_trips[df_trips['service_id'].isin(todays_services)]\n",
    "pd.options.display.max_rows = 10\n",
    "print(df_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gosh, that's a lot of services... What are RTTA_DEF and RTTA_REV... and are those CountryLink services??? Let's filter out some of these services as we're only going to analyse what is going on in the general Sydney commuter network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      route_id   service_id                        trip_id  trip_short_name\n",
      "0       BNK_2a   1260.122.4   1--A.1260.122.4.M.8.55188158              NaN\n",
      "1       BNK_2a  1260.122.48  1--A.1260.122.48.M.8.55188157              NaN\n",
      "2       BNK_2a  1260.122.64  1--A.1260.122.64.M.8.55188157              NaN\n",
      "3       BNK_2a   1260.122.8   1--A.1260.122.8.M.8.55188158              NaN\n",
      "6       BNK_2a  1603.103.60  1--A.1603.103.60.M.8.54724492              NaN\n",
      "...        ...          ...                            ...              ...\n",
      "57276    BMT_2   483.101.56   WN18.483.101.56.N.2.55278207              NaN\n",
      "57277    BMT_2   483.101.64   WN18.483.101.64.N.2.55278207              NaN\n",
      "57278    BMT_2    487.111.4    WN18.487.111.4.N.2.55308164              NaN\n",
      "57279    BMT_2   487.111.56   WN18.487.111.56.N.2.55308164              NaN\n",
      "57280    BMT_2   487.111.64   WN18.487.111.64.N.2.55308164              NaN\n",
      "\n",
      "[47941 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "ROUTES_TO_IGNORE = [\"CTY_NC1\", \"CTY_NC1a\", \"CTY_NC2\", \n",
    "                    \"CTY_NW1a\", \"CTY_NW1b\", \"CTY_NW1c\", \"CTY_NW1d\", \"CTY_NW2a\", \"CTY_NW2b\", \n",
    "                    \"CTY_S1a\", \"CTY_S1b\", \"CTY_S1c\", \"CTY_S1d\", \"CTY_S1e\", \"CTY_S1f\", \n",
    "                    \"CTY_S1g\", \"CTY_S1h\", \"CTY_S1i\", \n",
    "                    \"CTY_S2a\", \"CTY_S2b\", \"CTY_S2c\", \"CTY_S2d\", \"CTY_S2e\", \"CTY_S2f\", \n",
    "                    \"CTY_S2g\", \"CTY_S2h\", \"CTY_S2i\", \n",
    "                    \"CTY_W1a\", \"CTY_W1b\", \"CTY_W2a\", \"CTY_W2b\", \n",
    "                    \"HUN_1a\", \"HUN_1b\", \"HUN_2a\", \"HUN_2b\", \n",
    "                    \"RTTA_DEF\", \"RTTA_REV\"]\n",
    "df_trips = df_trips[~df_trips['route_id'].isin(ROUTES_TO_IGNORE)]\n",
    "print(df_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `stop_times.txt` file contains the stop files from across a number of days. We can filter out which services we want by only looking at trips that are running today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               trip_id arrival_time departure_time  stop_id\n",
      "10       1--A.1260.122.48.M.8.55188157     03:52:00       03:52:00  2144243\n",
      "11       1--A.1260.122.48.M.8.55188157     03:54:12       03:55:00  2141313\n",
      "12       1--A.1260.122.48.M.8.55188157     03:57:30       03:57:30   214063\n",
      "13       1--A.1260.122.48.M.8.55188157     03:58:42       03:58:42   214074\n",
      "14       1--A.1260.122.48.M.8.55188157     04:01:24       04:01:24  2135234\n",
      "...                                ...          ...            ...      ...\n",
      "1033592  WT28.1260.122.60.X.5.55187037     20:26:24       20:26:24   214072\n",
      "1033593  WT28.1260.122.60.X.5.55187037     20:27:36       20:29:12  2135232\n",
      "1033594  WT28.1260.122.60.X.5.55187037     20:31:24       20:31:24   213491\n",
      "1033595  WT28.1260.122.60.X.5.55187037     20:39:06       20:39:06  2015133\n",
      "1033596  WT28.1260.122.60.X.5.55187037     20:42:24       23:34:00  2000325\n",
      "\n",
      "[73135 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_stop_times = pd.read_csv(data_path + 'stop_times.txt', header=0,\n",
    "                            encoding='utf-8-sig',\n",
    "                            dtype={'stop_id': str},\n",
    "                            usecols=[\"trip_id\", \"arrival_time\", \"departure_time\", \"stop_id\"],\n",
    "                            parse_dates=['arrival_time', 'departure_time'])\n",
    "\n",
    "# remove any trips from stop_times that did NOT happen on this date\n",
    "df_filtered_stop_times = df_stop_times[df_stop_times['trip_id'].isin(df_filtered_trips['trip_id'])]\n",
    "print(df_filtered_stop_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing delay data\n",
    "In the archive, all of the responses to requests to the [Transport for NSW Open Data API](https://opendata.transport.nsw.gov.au) have been saved. As part of the repository, there is a Python task for making these requests every two minutes, 24 hours a day.\n",
    "This data is in the format according to [General Transit Feed Specification](https://developers.google.com/transit/) and can be parsed with the [GTFS python library](https://developers.google.com/transit/gtfs-realtime/examples/python-sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3515 trips\n"
     ]
    }
   ],
   "source": [
    "from google.transit import gtfs_realtime_pb2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.features.trip_objects import *\n",
    "import pickle\n",
    "\n",
    "# get all the delay files\n",
    "files_in_dir = sorted(glob.glob(data_path + '*.pickle'))\n",
    "\n",
    "merged_delays = dict()\n",
    "\n",
    "for delay_data_file in files_in_dir:\n",
    "    try:\n",
    "        current_delay_response = pickle.load(open(delay_data_file, \"rb\"))\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        continue  # next file\n",
    "\n",
    "    feed = gtfs_realtime_pb2.FeedMessage()\n",
    "    feed.ParseFromString(current_delay_response)\n",
    "    for entity in feed.entity:\n",
    "        if entity.HasField('trip_update') and len(entity.trip_update.stop_time_update) > 0:\n",
    "            trip_update = TripUpdate(entity.trip_update.trip.trip_id,\n",
    "                                     entity.trip_update.trip.route_id,\n",
    "                                     entity.trip_update.trip.schedule_relationship,\n",
    "                                     entity.trip_update.timestamp)\n",
    "\n",
    "            for stop_time_update in entity.trip_update.stop_time_update:\n",
    "                trip_update.stop_time_updates[stop_time_update.stop_id] \\\n",
    "                    = StopTimeUpdate(stop_time_update.stop_id,\n",
    "                                     stop_time_update.arrival.delay,\n",
    "                                     stop_time_update.departure.delay,\n",
    "                                     stop_time_update.schedule_relationship)\n",
    "\n",
    "            if trip_update is None:\n",
    "                print('trip update is none')\n",
    "            # merge with current trips\n",
    "            if trip_update.trip_id in merged_delays:\n",
    "                merged_delays[trip_update.trip_id] = \\\n",
    "                    merge_trips(merged_delays[trip_update.trip_id], trip_update)\n",
    "            else:\n",
    "                merged_delays[trip_update.trip_id] = trip_update\n",
    "\n",
    "print(\"Found \" + str(len(merged_delays)) + \" trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
